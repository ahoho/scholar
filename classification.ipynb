{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import file_handling as fh\n",
    "from run_scholar import (\n",
    "    load_word_counts, load_scholar_model, load_labels, get_minibatch, predict_label_probs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = \"imdb-k_50/learned_dev_no_bg-with_label\"\n",
    "DATA_DIR = \"../scholar/data/imdb/processed\"\n",
    "SPLIT = \"train\"\n",
    "\n",
    "checkpoint = torch.load(Path(MODEL_DIR, \"torch_model.pt\"), map_location=\"cpu\")\n",
    "options = checkpoint['options']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick look at accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.9679, 0.002768\n"
    }
   ],
   "source": [
    "train_accuracies = [\n",
    "    float(fh.read_text(Path(model_seed_dir, \"accuracy.train.txt\"))[0])\n",
    "    for model_seed_dir in Path(MODEL_DIR).glob(\"[0-9]*\")\n",
    "]\n",
    "print(f\"{np.mean(train_accuracies):0.4}, {np.std(train_accuracies):0.4}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading data\nLoaded 25000 documents with 5000 features\nFound 25000 non-empty documents\nLoading labels from ../scholar/data/imdb/processed/train.sentiment.csv\nFound 2 labels\n"
    }
   ],
   "source": [
    "vocab = fh.read_json(Path(DATA_DIR, \"train.vocab.json\"))\n",
    "test_X, _, test_row_selector, test_ids = load_word_counts(\n",
    "    DATA_DIR, SPLIT, vocab=vocab\n",
    ")\n",
    "test_labels, _, label_names, _ = load_labels(\n",
    "    DATA_DIR, SPLIT, test_row_selector, \"sentiment\",\n",
    ")\n",
    "test_topic_covars = None # for now\n",
    "n_test = test_X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create two versions of the \"prior covars\" that will index the deviation embeddings, one per label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Loading labels from ../scholar/data/imdb/processed/train.sentiment.csv\nFound 2 labels\n"
    }
   ],
   "source": [
    "deviation_indexer_neg = np.vstack([np.ones(n_test), np.zeros(n_test)]).T\n",
    "deviation_indexer_pos = np.vstack([np.zeros(n_test), np.ones(n_test)]).T\n",
    "_, _, deviation_covar_names, _ = load_labels(\n",
    "    DATA_DIR, \"train\", test_row_selector, options.deviation_embedding_covar\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dummy dictionary for model loading since embeddings will be loaded from the torch checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scholar import Scholar\n",
    "def load_scholar_model(inpath, embeddings=None, map_location=None):\n",
    "    \"\"\"\n",
    "    Load the Scholar model\n",
    "    \"\"\"\n",
    "    checkpoint = torch.load(inpath, map_location=map_location)\n",
    "    scholar_kwargs = checkpoint[\"scholar_kwargs\"]\n",
    "    scholar_kwargs[\"init_embeddings\"] = embeddings\n",
    "    if map_location == 'cpu':\n",
    "        scholar_kwargs['device'] = None\n",
    "\n",
    "    model = Scholar(**scholar_kwargs)\n",
    "    model._model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    model.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "\n",
    "    return model, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {} \n",
    "if options.background_embeddings:\n",
    "    embeddings[\"background\"] = None, True\n",
    "if options.deviation_embeddings:\n",
    "    for name in deviation_covar_names:\n",
    "        embeddings[name] = None, True\n",
    "model, _ = load_scholar_model(\n",
    "    Path(MODEL_DIR, \"torch_model.pt\"), embeddings=embeddings, map_location='cpu'\n",
    ")\n",
    "model.eval()           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate reconstruction errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recon_errors(model, X, PC, TC, batch_size):\n",
    "    \"\"\"\n",
    "    Get the reconstruction errors for each doc in the dataset\n",
    "    \"\"\"\n",
    "    n_items, _ = X.shape\n",
    "    n_batches = int(np.ceil(n_items / batch_size))\n",
    "    Y = None\n",
    "    recon_errors = []\n",
    "    for i in range(n_batches):\n",
    "        batch_xs, _, batch_pcs, batch_tcs = get_minibatch(\n",
    "            X, Y, PC, TC, i, batch_size\n",
    "        )\n",
    "        \n",
    "        batch_size = model.get_batch_size(batch_xs)\n",
    "        if batch_size == 1:\n",
    "            batch_xs = np.expand_dims(batch_xs, axis=0)\n",
    "        if batch_pcs is not None and batch_size == 1:\n",
    "            batch_pcs = np.expand_dims(batch_pcs, axis=0)\n",
    "        if batch_tcs is not None and batch_size == 1:\n",
    "            batch_tcs = np.expand_dims(batch_tcs, axis=0)\n",
    "        \n",
    "        batch_xs = torch.Tensor(batch_xs).to(model.device)\n",
    "        if batch_pcs is not None:\n",
    "            batch_pcs = torch.Tensor(batch_pcs).to(model.device)\n",
    "        if batch_tcs is not None:\n",
    "            batch_tcs = torch.Tensor(batch_tcs).to(model.device)\n",
    "\n",
    "        # dummy Ys\n",
    "        batch_ys = np.zeros((batch_size, model._model.n_labels), np.float32)\n",
    "        batch_ys = torch.Tensor(batch_ys).to(model.device)\n",
    "\n",
    "        _, X_recon, _ = model._model(\n",
    "            batch_xs,\n",
    "            batch_ys,\n",
    "            batch_pcs,\n",
    "            batch_tcs,\n",
    "            compute_loss=False,\n",
    "            var_scale=1.0,\n",
    "            eta_bn_prop=0.0,\n",
    "        )\n",
    "        error = -(batch_xs * (X_recon + 1e-10).log()).sum(1).detach().numpy()\n",
    "        recon_errors.append(error)\n",
    "        \n",
    "    return np.concatenate(recon_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_neg = get_recon_errors(\n",
    "    model, test_X, PC=deviation_indexer_neg, TC=None, batch_size=512\n",
    ")\n",
    "error_pos = get_recon_errors(\n",
    "    model, test_X, PC=deviation_indexer_pos, TC=None, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['neg', 'pos'], dtype='object')"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Error on negative documents\n680.2247\n688.6816\nError on positive documents\n703.16034\n694.81805\n"
    }
   ],
   "source": [
    "print(\"Error on negative documents\")\n",
    "print(error_neg[test_labels[:, 0] == 1].mean())\n",
    "print(error_pos[test_labels[:, 0] == 1].mean())\n",
    "\n",
    "print(\"Error on positive documents\")\n",
    "print(error_neg[test_labels[:, 0] == 0].mean())\n",
    "print(error_pos[test_labels[:, 0] == 0].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True,  True, ..., False, False, False])"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(error_pos > error_neg) & (test_labels[:, 0] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Negative recall\n0.90216\nPositive recall\n0.88176\n"
    }
   ],
   "source": [
    "print(\"Negative recall\")\n",
    "print(\n",
    "    ((error_neg < error_pos) & (test_labels[:, 0] == 1)).sum() \n",
    "    / (test_labels[:, 0] == 1).sum()\n",
    ")\n",
    "print(\"Positive recall\")\n",
    "print(\n",
    "    ((error_neg > error_pos) & (test_labels[:, 0] == 0)).sum()\n",
    "    / (test_labels[:, 0] == 0).sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use reconstruction errors in prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "deviation_indexer_from_recon_loss = np.array([\n",
    "    1 * (error_neg < error_pos), 1 * (error_neg > error_pos)\n",
    "]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs = predict_label_probs(\n",
    "    model, test_X, PC=deviation_indexer_from_recon_loss, TC=None, batch_size=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.8866"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred_probs.argmax(1) == test_labels[:, 1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'imdb-k_50/learned_dev_no_bg-with_label'"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'imdb-k_50/baseline-no_dev_learned_bg-with_label', 0.897 train, 0.849 test\n",
    "'imdb-k_50/learned_dev_no_bg-with_label', 0.8867 train, \n",
    "\n",
    "\n",
    "\n",
    "x"
   ]
  }
 ]
}